{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1351797,"sourceType":"datasetVersion","datasetId":786787}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv2D , MaxPooling2D , Flatten , Dropout , BatchNormalization\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator , load_img\nfrom keras import regularizers\nfrom keras.optimizers import Adam, RMSprop, SGD\nfrom tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, TensorBoard, EarlyStopping, ReduceLROnPlateau\nimport datetime\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,confusion_matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **1. Data Import**","metadata":{}},{"cell_type":"code","source":"train_dir = '../input/fer2013/train/'\ntest_dir = '../input/fer2013/test/'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Data Check**","metadata":{}},{"cell_type":"code","source":"row, col = 48, 48\nclasses = 7\n\ndef data_count(path, set_):\n    dict_ = {}\n    \n    for expression in os.listdir(path):\n        dir_ = path + expression\n        dict_[expression] = len(os.listdir(dir_))\n        \n    df = pd.DataFrame(dict_, index=[set_])\n    return df\n\ntrain_count = data_count(train_dir, 'train')\ntest_count = data_count(test_dir, 'test')\n\nprint(train_count)\nprint(test_count)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_count.transpose().plot(kind='bar', title = 'Train data Count')\ntest_count.transpose().plot(kind='bar', title = 'Test data Count')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **2. Data Visualization**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(14,22))\ni = 1\nfor expression in os.listdir(train_dir):\n    img = load_img((train_dir + expression +'/'+ os.listdir(train_dir + expression)[1]))\n    plt.subplot(1,7,i)\n    plt.imshow(img)\n    plt.title(expression)\n    plt.axis('off')\n    i += 1\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **3. Modeling**","metadata":{}},{"cell_type":"code","source":"train_datagen = ImageDataGenerator(\n    width_shift_range = 0.1,        \n    height_shift_range = 0.1,       \n    horizontal_flip = True,         \n    rescale = 1./255,               \n    validation_split = 0.2          \n)\n\ntrain_set = train_datagen.flow_from_directory(\n    directory = train_dir,           \n    target_size = (48, 48),          \n    batch_size = 64,                 \n    color_mode = \"grayscale\",        \n    class_mode = \"categorical\",      \n    subset = \"training\"              \n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_datagen = ImageDataGenerator(rescale=1./255,\n                                 validation_split = 0.2 \n                                 )\n\ntest_set = test_datagen.flow_from_directory(\n    directory = test_dir,            \n    target_size = (48, 48),          \n    batch_size = 64,                 \n    color_mode = \"grayscale\",        \n    class_mode = \"categorical\",      \n    subset = \"validation\"            \n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_set.class_indices","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**CNN Model**","metadata":{}},{"cell_type":"code","source":"model = Sequential()\n\n# Add a convolutional layer with 32 filters, 3x3 kernel size, and relu activation function\nmodel.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48,48,1)))\n# Add a batch normalization layer\nmodel.add(BatchNormalization())\n# Add a second convolutional layer with 64 filters, 3x3 kernel size, and relu activation function\nmodel.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n# Add a second batch normalization layer\nmodel.add(BatchNormalization())\n# Add a max pooling layer with 2x2 pool size\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n# Add a dropout layer with 0.25 dropout rate\nmodel.add(Dropout(0.25))\n\n# Add a third convolutional layer with 128 filters, 3x3 kernel size, and relu activation function\nmodel.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n# Add a third batch normalization layer\nmodel.add(BatchNormalization())\n# Add a fourth convolutional layer with 128 filters, 3x3 kernel size, and relu activation function\nmodel.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n# Add a fourth batch normalization layer\nmodel.add(BatchNormalization())\n# Add a max pooling layer with 2x2 pool size\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n# Add a dropout layer with 0.25 dropout rate\nmodel.add(Dropout(0.25))\n\n# Add a fifth convolutional layer with 256 filters, 3x3 kernel size, and relu activation function\nmodel.add(Conv2D(256, kernel_size=(3, 3), activation='relu'))\n# Add a fifth batch normalization layer\nmodel.add(BatchNormalization())\n# Add a sixth convolutional layer with 256 filters, 3x3 kernel size, and relu activation function\nmodel.add(Conv2D(256, kernel_size=(3, 3), activation='relu'))\n# Add a sixth batch normalization layer\nmodel.add(BatchNormalization())\n# Add a max pooling layer with 2x2 pool size\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n# Add a dropout layer with 0.25 dropout rate\nmodel.add(Dropout(0.25))\n\n# Flatten the output of the convolutional layers\nmodel.add(Flatten())\n# Add a dense layer with 256 neurons and relu activation function\nmodel.add(Dense(256, activation='relu'))\n# Add a seventh batch normalization layer\nmodel.add(BatchNormalization())\n# Add a dropout layer with 0.5 dropout rate\nmodel.add(Dropout(0.5))\n# Add a dense layer with 7 neurons (one for each class) and softmax activation function\nmodel.add(Dense(7, activation='softmax'))\n\n# Compile the model with categorical cross-entropy loss, adam optimizer, and accuracy metric\nmodel.compile(loss=\"categorical_crossentropy\", optimizer= tf.keras.optimizers.Adam(learning_rate=0.0001), metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint_callback = ModelCheckpoint(\n    filepath='model.weights.h5',\n    monitor='val_accuracy',\n    save_best_only=True,\n    save_weights_only=True,\n    mode='max',\n    verbose=1\n)\n\n# Train the model with the callback\nhistory = model.fit(\n    train_set,\n    steps_per_epoch=len(train_set),\n    epochs=50,\n    validation_data=test_set,\n    validation_steps=len(test_set),\n    callbacks=[checkpoint_callback]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(train_loss) + 1)\nplt.plot(epochs, train_loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nplt.plot(epochs, train_acc, 'bo', label='Training accuracy')\nplt.plot(epochs, val_acc, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validation_labels = test_set.classes\nvalidation_pred_probs = model.predict(test_set)\nvalidation_pred_labels = np.argmax(validation_pred_probs, axis=1)\n\n# Compute the confusion matrix\nconfusion_mtx = confusion_matrix(validation_labels, validation_pred_labels)\nclass_names = list(train_set.class_indices.keys())\nsns.set()\nsns.heatmap(confusion_mtx, annot=True, fmt='d', cmap='YlGnBu', \n            xticklabels=class_names, yticklabels=class_names)\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Please leave your advice in the comments. Thank you**","metadata":{}}]}